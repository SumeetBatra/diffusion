{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shashank/research/qd/diffusion_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "project_root = os.path.join(str(Path.home()), 'research/qd/diffusion_models')\n",
    "os.chdir(project_root)\n",
    "%pwd # should be PPGA root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/miniconda3/envs/qd/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset fashion_mnist (/home/shashank/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 571.47it/s]\n",
      "WARNING:evotorch:The logger is already configured. The default configuration will not be applied. Call `set_default_logger_config` with `override=True` to override the current configuration.\n",
      "100%|██████████| 7115/7115 [00:14<00:00, 480.85it/s]\n",
      "100%|██████████| 791/791 [00:01<00:00, 439.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from autoencoders.transformer_autoencoder import AutoEncoder\n",
    "from autoencoders.policy_hyperautoencoder import HyperAutoEncoder\n",
    "from autoencoders.conv_autoencoder import AutoEncoder as ConvVAE\n",
    "from dataset.mnist_fashion_dataset import dataloader\n",
    "from losses.loss_functions import normal_kl\n",
    "from losses.contperceptual import LPIPSWithDiscriminator\n",
    "\n",
    "from dataset.policy_dataset import e_data_loader_train, actor_cfg\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'module name can\\'t contain \".\", got: actor_mean.0.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m# model = AutoEncoder(emb_channels=8, z_channels=4)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# model = ConvVAE()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m HyperAutoEncoder(actor_cfg, emb_channels\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, z_channels\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/autoencoders/policy_hyperautoencoder.py:136\u001b[0m, in \u001b[0;36mHyperAutoEncoder.__init__\u001b[0;34m(self, actor_cfg, emb_channels, z_channels)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    135\u001b[0m \u001b[39m# self.encoder = Encoder(channels=2, channel_multipliers=[1, 2, 4, 8], n_resnet_blocks=3, in_channels=1, z_channels=z_channels)\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m ModelEncoder(actor_cfg, z_channels)\n\u001b[1;32m    137\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39m# self.decoder = Decoder(channels=2, channel_multipliers=[8, 4, 2, 1], n_resnet_blocks=3, out_channels=1, z_channels=z_channels)\u001b[39;00m\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/autoencoders/policy_hyperautoencoder.py:233\u001b[0m, in \u001b[0;36mModelEncoder.__init__\u001b[0;34m(self, actor_cfg, z_channels)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnns[name], op_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_fc_backbone(shape)\n\u001b[1;32m    231\u001b[0m     total_op_shape \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mprod(op_shape)\n\u001b[0;32m--> 233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnns \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mModuleDict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcnns)\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(total_op_shape, \u001b[39m8\u001b[39m\u001b[39m*\u001b[39m\u001b[39m4\u001b[39m\u001b[39m*\u001b[39mz_channels)\n",
      "File \u001b[0;32m~/miniconda3/envs/qd/lib/python3.9/site-packages/torch/nn/modules/container.py:412\u001b[0m, in \u001b[0;36mModuleDict.__init__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39msuper\u001b[39m(ModuleDict, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(modules)\n",
      "File \u001b[0;32m~/miniconda3/envs/qd/lib/python3.9/site-packages/torch/nn/modules/container.py:488\u001b[0m, in \u001b[0;36mModuleDict.update\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(modules, (OrderedDict, ModuleDict, container_abcs\u001b[39m.\u001b[39mMapping)):\n\u001b[1;32m    487\u001b[0m     \u001b[39mfor\u001b[39;00m key, module \u001b[39min\u001b[39;00m modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 488\u001b[0m         \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m module\n\u001b[1;32m    489\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[39m# modules here can be a list with two items\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[39mfor\u001b[39;00m j, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(modules):\n",
      "File \u001b[0;32m~/miniconda3/envs/qd/lib/python3.9/site-packages/torch/nn/modules/container.py:419\u001b[0m, in \u001b[0;36mModuleDict.__setitem__\u001b[0;34m(self, key, module)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m, module: Module) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(key, module)\n",
      "File \u001b[0;32m~/miniconda3/envs/qd/lib/python3.9/site-packages/torch/nn/modules/module.py:452\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mattribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m    451\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m name:\n\u001b[0;32m--> 452\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule name can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m, got: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m    453\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule name can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be empty string \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'module name can\\'t contain \".\", got: actor_mean.0.weight'"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = AutoEncoder(emb_channels=8, z_channels=4)\n",
    "# model = ConvVAE()\n",
    "model = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "mse_loss_func = torch.nn.MSELoss()\n",
    "kl_loss_coef = 1e-6\n",
    "\n",
    "model_checkpoint_folder = Path('./checkpoints2')\n",
    "model_checkpoint_folder.mkdir(exist_ok=True)\n",
    "\n",
    "disc_start = 50001\n",
    "kl_weight = 1e-6\n",
    "disc_weight = 0.5\n",
    "# loss_func = LPIPSWithDiscriminator(disc_start, kl_weight=kl_weight, disc_weight=disc_weight)\n",
    "# optimizer2 = Adam(loss_func.discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'{epoch=}')\n",
    "    print(f'{global_step=}')\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(e_data_loader_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # batch = batch['pixel_values'].to(device)\n",
    "        batch = batch[0]\n",
    "\n",
    "        # img_out, posterior = model(batch)\n",
    "        img_out = model(batch)\n",
    "        # loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "        # loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "        pred_weights_dict = {}\n",
    "        for out_ in img_out:\n",
    "            for name, param in out_.named_parameters():\n",
    "                if name not in pred_weights_dict:\n",
    "                    pred_weights_dict[name] = []\n",
    "                pred_weights_dict[name].append(param)\n",
    "                \n",
    "\n",
    "        # loss is MSE loss\n",
    "        mse_loss = 0\n",
    "        for key in pred_weights_dict:\n",
    "            mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), batch[key])\n",
    "\n",
    "        # loss = mse_loss_func(batch, img_out) + kl_loss_coef * posterior.kl().mean()\n",
    "        # loss = mse_loss + kl_loss_coef * posterior.kl().mean()\n",
    "        loss = mse_loss \n",
    "\n",
    "        loss.backward()\n",
    "        # if step % 100 == 0:\n",
    "            # print(f'Loss: {loss.item()}, MSE: {mse_loss.item()}, KL: {posterior.kl().mean().item()}')\n",
    "            # print(f'Loss: {loss.item()}, MSE: {mse_loss.item()}')\n",
    "            # print(f'grad norm: {grad_norm(model)}')\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    global_step += step\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}, MSE: {epoch_loss/len(e_data_loader_train)}')\n",
    "print('Saving final model checkpoint...')\n",
    "torch.save(model.state_dict(), os.path.join(str(model_checkpoint_folder), 'autoencoder.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model2 = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1967, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cp = f\"./{model_checkpoint_folder}/autoencoder.pt\"\n",
    "model2.load_state_dict(torch.load(model_cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "# model2.decoder.load_state_dict(model.decoder.state_dict())\n",
    "# model2.encoder.load_state_dict(model.encoder.state_dict())\n",
    "model2.encoder = model.encoder\n",
    "model2.decoder = model.decoder\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0069, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c437a21b275005b244436c232defa84105462a85d5e3990796798eae336d0140"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shashank/research/qd/diffusion_models'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "project_root = os.path.join(str(Path.home()), 'research/qd/diffusion_models')\n",
    "os.chdir(project_root)\n",
    "%pwd # should be PPGA root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashank/miniconda3/envs/qd/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset fashion_mnist (/home/shashank/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 121.52it/s]\n",
      "WARNING:evotorch:The logger is already configured. The default configuration will not be applied. Call `set_default_logger_config` with `override=True` to override the current configuration.\n",
      "100%|██████████| 7115/7115 [00:16<00:00, 434.63it/s]\n",
      "100%|██████████| 791/791 [00:01<00:00, 408.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from autoencoders.transformer_autoencoder import AutoEncoder\n",
    "from autoencoders.policy_hyperautoencoder import HyperAutoEncoder\n",
    "from autoencoders.conv_autoencoder import AutoEncoder as ConvVAE\n",
    "from dataset.mnist_fashion_dataset import dataloader\n",
    "from losses.loss_functions import normal_kl\n",
    "from losses.contperceptual import LPIPSWithDiscriminator\n",
    "\n",
    "from dataset.policy_dataset import e_data_loader_train, actor_cfg\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "global_step=0\n",
      "Epoch: 0, Loss: 0.09606116265058517, MSE: 0.13802241260578874, KL: 0.6087906640877417\n",
      "epoch=1\n",
      "global_step=217\n",
      "Epoch: 1, Loss: 0.07541848719120026, MSE: 0.0829739210392357, KL: 0.09106343304482076\n",
      "epoch=2\n",
      "global_step=434\n",
      "Epoch: 2, Loss: 0.05970236659049988, MSE: 0.06479385372148741, KL: 0.058478007691169\n",
      "epoch=3\n",
      "global_step=651\n",
      "Epoch: 3, Loss: 0.041415076702833176, MSE: 0.05211580288382845, KL: 0.04805186746317312\n",
      "epoch=4\n",
      "global_step=868\n",
      "Epoch: 4, Loss: 0.032211948186159134, MSE: 0.044128263386291104, KL: 0.04111457278593144\n",
      "epoch=5\n",
      "global_step=1085\n",
      "Epoch: 5, Loss: 0.03664721921086311, MSE: 0.04037878909276439, KL: 0.03691826523238913\n",
      "epoch=6\n",
      "global_step=1302\n",
      "Epoch: 6, Loss: 0.038538530468940735, MSE: 0.03897500198778756, KL: 0.03234437958192114\n",
      "epoch=7\n",
      "global_step=1519\n",
      "Epoch: 7, Loss: 0.03515780344605446, MSE: 0.038572780907564205, KL: 0.03849451595900256\n",
      "epoch=8\n",
      "global_step=1736\n",
      "Epoch: 8, Loss: 0.028991831466555595, MSE: 0.03845961548285473, KL: 0.033712919442615376\n",
      "epoch=9\n",
      "global_step=1953\n",
      "Epoch: 9, Loss: 0.033192649483680725, MSE: 0.03838309491856383, KL: 0.04143237854875283\n",
      "epoch=10\n",
      "global_step=2170\n",
      "Epoch: 10, Loss: 0.03357716277241707, MSE: 0.038353662083455184, KL: 0.04657535079798294\n",
      "epoch=11\n",
      "global_step=2387\n",
      "Epoch: 11, Loss: 0.05243420973420143, MSE: 0.038303713818345596, KL: 0.07647169588150782\n",
      "epoch=12\n",
      "global_step=2604\n",
      "Epoch: 12, Loss: 0.0306868776679039, MSE: 0.03812384828412478, KL: 0.1770960751305753\n",
      "epoch=13\n",
      "global_step=2821\n",
      "Epoch: 13, Loss: 0.03688160330057144, MSE: 0.03773674058702287, KL: 0.39200921442120445\n",
      "epoch=14\n",
      "global_step=3038\n",
      "Epoch: 14, Loss: 0.04228692874312401, MSE: 0.03724460872830054, KL: 0.5163682517723753\n",
      "epoch=15\n",
      "global_step=3255\n",
      "Epoch: 15, Loss: 0.04095790907740593, MSE: 0.03656997499184324, KL: 0.5948013851265295\n",
      "epoch=16\n",
      "global_step=3472\n",
      "Epoch: 16, Loss: 0.037543799728155136, MSE: 0.03379216321120295, KL: 0.6819258462808547\n",
      "epoch=17\n",
      "global_step=3689\n",
      "Epoch: 17, Loss: 0.025708116590976715, MSE: 0.0308345614530898, KL: 1.251805993805238\n",
      "epoch=18\n",
      "global_step=3906\n",
      "Epoch: 18, Loss: 0.026925943791866302, MSE: 0.028910845526656428, KL: 1.3631885408193147\n",
      "epoch=19\n",
      "global_step=4123\n",
      "Epoch: 19, Loss: 0.03889160230755806, MSE: 0.02772653782142138, KL: 1.3715919310893487\n",
      "epoch=20\n",
      "global_step=4340\n",
      "Epoch: 20, Loss: 0.03142498433589935, MSE: 0.02652826659176328, KL: 1.394453111660043\n",
      "epoch=21\n",
      "global_step=4557\n",
      "Epoch: 21, Loss: 0.018858827650547028, MSE: 0.025698551284405617, KL: 1.3798595578025241\n",
      "epoch=22\n",
      "global_step=4774\n",
      "Epoch: 22, Loss: 0.02117781899869442, MSE: 0.025355250993316328, KL: 1.4406633684667973\n",
      "epoch=23\n",
      "global_step=4991\n",
      "Epoch: 23, Loss: 0.023666346445679665, MSE: 0.024940735207648453, KL: 1.363799599332547\n",
      "epoch=24\n",
      "global_step=5208\n",
      "Epoch: 24, Loss: 0.02594052627682686, MSE: 0.02461550791465908, KL: 1.3626453026172218\n",
      "epoch=25\n",
      "global_step=5425\n",
      "Epoch: 25, Loss: 0.026340702548623085, MSE: 0.024249877276565503, KL: 1.3242618172267162\n",
      "epoch=26\n",
      "global_step=5642\n",
      "Epoch: 26, Loss: 0.02579227089881897, MSE: 0.0239037056838017, KL: 1.3511609772408226\n",
      "epoch=27\n",
      "global_step=5859\n",
      "Epoch: 27, Loss: 0.027047187089920044, MSE: 0.023653314730853116, KL: 1.2668721723037029\n",
      "epoch=28\n",
      "global_step=6076\n",
      "Epoch: 28, Loss: 0.023078570142388344, MSE: 0.02321741977757817, KL: 1.3297269312203477\n",
      "epoch=29\n",
      "global_step=6293\n",
      "Epoch: 29, Loss: 0.020341787487268448, MSE: 0.022949675080973075, KL: 1.3124344715968184\n",
      "epoch=30\n",
      "global_step=6510\n",
      "Epoch: 30, Loss: 0.025279859080910683, MSE: 0.02258625994820934, KL: 1.4057575946144008\n",
      "epoch=31\n",
      "global_step=6727\n",
      "Epoch: 31, Loss: 0.027658291161060333, MSE: 0.022340554479133645, KL: 1.429215229774287\n",
      "epoch=32\n",
      "global_step=6944\n",
      "Epoch: 32, Loss: 0.02261950820684433, MSE: 0.022135996610062932, KL: 1.2360829031180351\n",
      "epoch=33\n",
      "global_step=7161\n",
      "Epoch: 33, Loss: 0.023162197321653366, MSE: 0.021986828185617924, KL: 1.3493982541178344\n",
      "epoch=34\n",
      "global_step=7378\n",
      "Epoch: 34, Loss: 0.025292446836829185, MSE: 0.021890460573341868, KL: 1.444194444617547\n",
      "epoch=35\n",
      "global_step=7595\n",
      "Epoch: 35, Loss: 0.02885816991329193, MSE: 0.021776614873029225, KL: 1.3417496198634489\n",
      "epoch=36\n",
      "global_step=7812\n",
      "Epoch: 36, Loss: 0.022786976769566536, MSE: 0.021752616591375628, KL: 1.4104347115536349\n",
      "epoch=37\n",
      "global_step=8029\n",
      "Epoch: 37, Loss: 0.022247640416026115, MSE: 0.021494377498438053, KL: 1.3593425806782662\n",
      "epoch=38\n",
      "global_step=8246\n",
      "Epoch: 38, Loss: 0.025396212935447693, MSE: 0.02136879634761482, KL: 1.4050308668968874\n",
      "epoch=39\n",
      "global_step=8463\n",
      "Epoch: 39, Loss: 0.021959060803055763, MSE: 0.021252355794635934, KL: 1.3624029483543623\n",
      "epoch=40\n",
      "global_step=8680\n",
      "Epoch: 40, Loss: 0.022848118096590042, MSE: 0.021094841340027, KL: 1.321782696957982\n",
      "epoch=41\n",
      "global_step=8897\n",
      "Epoch: 41, Loss: 0.02245304174721241, MSE: 0.02099288722773621, KL: 1.470069533846247\n",
      "epoch=42\n",
      "global_step=9114\n",
      "Epoch: 42, Loss: 0.020615437999367714, MSE: 0.020808818455307035, KL: 1.309421624475663\n",
      "epoch=43\n",
      "global_step=9331\n",
      "Epoch: 43, Loss: 0.01862270198762417, MSE: 0.020834724623880804, KL: 1.3380736398314117\n",
      "epoch=44\n",
      "global_step=9548\n",
      "Epoch: 44, Loss: 0.024151403456926346, MSE: 0.020621387146994335, KL: 1.3023552360064392\n",
      "epoch=45\n",
      "global_step=9765\n",
      "Epoch: 45, Loss: 0.026056475937366486, MSE: 0.02048442754552725, KL: 1.4845788755684817\n",
      "epoch=46\n",
      "global_step=9982\n",
      "Epoch: 46, Loss: 0.024850748479366302, MSE: 0.020360742605915038, KL: 1.4172399389224315\n",
      "epoch=47\n",
      "global_step=10199\n",
      "Epoch: 47, Loss: 0.024455519393086433, MSE: 0.020134951432295347, KL: 1.5303465747231737\n",
      "epoch=48\n",
      "global_step=10416\n",
      "Epoch: 48, Loss: 0.02469141036272049, MSE: 0.019745187060651155, KL: 1.430685360326406\n",
      "epoch=49\n",
      "global_step=10633\n",
      "Epoch: 49, Loss: 0.015557019039988518, MSE: 0.01931304228278475, KL: 1.3363644296491364\n",
      "epoch=50\n",
      "global_step=10850\n",
      "Epoch: 50, Loss: 0.022682975977659225, MSE: 0.019005904341577937, KL: 1.398337556339732\n",
      "epoch=51\n",
      "global_step=11067\n",
      "Epoch: 51, Loss: 0.019779834896326065, MSE: 0.01859711155021956, KL: 1.521324631817844\n",
      "epoch=52\n",
      "global_step=11284\n",
      "Epoch: 52, Loss: 0.021990669891238213, MSE: 0.018514300401775388, KL: 1.4228985058058292\n",
      "epoch=53\n",
      "global_step=11501\n",
      "Epoch: 53, Loss: 0.02021818421781063, MSE: 0.018150011739702127, KL: 1.358687620165698\n",
      "epoch=54\n",
      "global_step=11718\n",
      "Epoch: 54, Loss: 0.01925991289317608, MSE: 0.01808700305110681, KL: 1.4061013940004035\n",
      "epoch=55\n",
      "global_step=11935\n",
      "Epoch: 55, Loss: 0.018158528953790665, MSE: 0.017913046066538183, KL: 1.3943122381327349\n",
      "epoch=56\n",
      "global_step=12152\n",
      "Epoch: 56, Loss: 0.018514353781938553, MSE: 0.017821204989567536, KL: 1.3978053883675041\n",
      "epoch=57\n",
      "global_step=12369\n",
      "Epoch: 57, Loss: 0.023604266345500946, MSE: 0.01765048500458035, KL: 1.4838328837254726\n",
      "epoch=58\n",
      "global_step=12586\n",
      "Epoch: 58, Loss: 0.01579524762928486, MSE: 0.01771681077795428, KL: 1.4838224877047976\n",
      "epoch=59\n",
      "global_step=12803\n",
      "Epoch: 59, Loss: 0.021752070635557175, MSE: 0.01759701830104267, KL: 1.44148624845601\n",
      "epoch=60\n",
      "global_step=13020\n",
      "Epoch: 60, Loss: 0.019826583564281464, MSE: 0.017629046950044983, KL: 1.4801739487079306\n",
      "epoch=61\n",
      "global_step=13237\n",
      "Epoch: 61, Loss: 0.01969962567090988, MSE: 0.017671083783679598, KL: 1.486555385822003\n",
      "epoch=62\n",
      "global_step=13454\n",
      "Epoch: 62, Loss: 0.01874600350856781, MSE: 0.01748947837654877, KL: 1.4687988296014454\n",
      "epoch=63\n",
      "global_step=13671\n",
      "Epoch: 63, Loss: 0.01597728580236435, MSE: 0.0175311100592307, KL: 1.5035106958052433\n",
      "epoch=64\n",
      "global_step=13888\n",
      "Epoch: 64, Loss: 0.02294255420565605, MSE: 0.017611662921316308, KL: 1.4182723389442908\n",
      "epoch=65\n",
      "global_step=14105\n",
      "Epoch: 65, Loss: 0.016332941129803658, MSE: 0.017531927388776607, KL: 1.3291560290124984\n",
      "epoch=66\n",
      "global_step=14322\n",
      "Epoch: 66, Loss: 0.02047344110906124, MSE: 0.01756734833753574, KL: 1.3365400891511812\n",
      "epoch=67\n",
      "global_step=14539\n",
      "Epoch: 67, Loss: 0.022056803107261658, MSE: 0.01760037084870519, KL: 1.4174320327989551\n",
      "epoch=68\n",
      "global_step=14756\n",
      "Epoch: 68, Loss: 0.022327901795506477, MSE: 0.0174915882365761, KL: 1.3933205265517628\n",
      "epoch=69\n",
      "global_step=14973\n",
      "Epoch: 69, Loss: 0.014323100447654724, MSE: 0.01758791279867975, KL: 1.4663170874802345\n",
      "epoch=70\n",
      "global_step=15190\n",
      "Epoch: 70, Loss: 0.02052393928170204, MSE: 0.017570469438722102, KL: 1.3825295659106807\n",
      "epoch=71\n",
      "global_step=15407\n",
      "Epoch: 71, Loss: 0.024541756138205528, MSE: 0.017546382788226966, KL: 1.3718825097819534\n",
      "epoch=72\n",
      "global_step=15624\n",
      "Epoch: 72, Loss: 0.02198115736246109, MSE: 0.017554584595827608, KL: 1.4717147555373131\n",
      "epoch=73\n",
      "global_step=15841\n",
      "Epoch: 73, Loss: 0.01804559864103794, MSE: 0.017564389431729503, KL: 1.4737187475991358\n",
      "epoch=74\n",
      "global_step=16058\n",
      "Epoch: 74, Loss: 0.015694517642259598, MSE: 0.017515399420651, KL: 1.4363660815112087\n",
      "epoch=75\n",
      "global_step=16275\n",
      "Epoch: 75, Loss: 0.017452821135520935, MSE: 0.017651167249631717, KL: 1.3377094531961538\n",
      "epoch=76\n",
      "global_step=16492\n",
      "Epoch: 76, Loss: 0.0165547002106905, MSE: 0.017674434810466724, KL: 1.509964860052964\n",
      "epoch=77\n",
      "global_step=16709\n",
      "Epoch: 77, Loss: 0.019167345017194748, MSE: 0.017517325737505057, KL: 1.5145007743747956\n",
      "epoch=78\n",
      "global_step=16926\n",
      "Epoch: 78, Loss: 0.020183002576231956, MSE: 0.017454701411717254, KL: 1.44076598620196\n",
      "epoch=79\n",
      "global_step=17143\n",
      "Epoch: 79, Loss: 0.016938360407948494, MSE: 0.017503628520072874, KL: 1.4124507499397347\n",
      "epoch=80\n",
      "global_step=17360\n",
      "Epoch: 80, Loss: 0.017162170261144638, MSE: 0.017579294553180354, KL: 1.5471605370761057\n",
      "epoch=81\n",
      "global_step=17577\n",
      "Epoch: 81, Loss: 0.017221709713339806, MSE: 0.017517206194989997, KL: 1.4241650955871157\n",
      "epoch=82\n",
      "global_step=17794\n",
      "Epoch: 82, Loss: 0.0185427013784647, MSE: 0.01756065504181139, KL: 1.284703361126808\n",
      "epoch=83\n",
      "global_step=18011\n",
      "Epoch: 83, Loss: 0.020040838047862053, MSE: 0.017464813900205794, KL: 1.587914051881077\n",
      "epoch=84\n",
      "global_step=18228\n",
      "Epoch: 84, Loss: 0.01748795621097088, MSE: 0.01754360742132188, KL: 1.4197405674452082\n",
      "epoch=85\n",
      "global_step=18445\n",
      "Epoch: 85, Loss: 0.018879225477576256, MSE: 0.01752387142287345, KL: 1.353121290376427\n",
      "epoch=86\n",
      "global_step=18662\n",
      "Epoch: 86, Loss: 0.01715496927499771, MSE: 0.017447721465157533, KL: 1.4825063045008466\n",
      "epoch=87\n",
      "global_step=18879\n",
      "Epoch: 87, Loss: 0.01628931425511837, MSE: 0.01747089091671306, KL: 1.335807153459536\n",
      "epoch=88\n",
      "global_step=19096\n",
      "Epoch: 88, Loss: 0.02057190239429474, MSE: 0.017399584460114942, KL: 1.4764358250236294\n",
      "epoch=89\n",
      "global_step=19313\n",
      "Epoch: 89, Loss: 0.016348274424672127, MSE: 0.01755294760842936, KL: 1.4446420405709415\n",
      "epoch=90\n",
      "global_step=19530\n",
      "Epoch: 90, Loss: 0.018231408670544624, MSE: 0.017611562244491567, KL: 1.24627206515555\n",
      "epoch=91\n",
      "global_step=19747\n",
      "Epoch: 91, Loss: 0.01870270073413849, MSE: 0.01753298919049835, KL: 1.382289933980605\n",
      "epoch=92\n",
      "global_step=19964\n",
      "Epoch: 92, Loss: 0.017040569335222244, MSE: 0.017594198256286733, KL: 1.4071572183742436\n",
      "epoch=93\n",
      "global_step=20181\n",
      "Epoch: 93, Loss: 0.021475497633218765, MSE: 0.017660783287650403, KL: 1.5114622962310773\n",
      "epoch=94\n",
      "global_step=20398\n",
      "Epoch: 94, Loss: 0.02194886840879917, MSE: 0.017488383350534997, KL: 1.352393410894849\n",
      "epoch=95\n",
      "global_step=20615\n",
      "Epoch: 95, Loss: 0.016202162951231003, MSE: 0.017603997564062886, KL: 1.418866787891869\n",
      "epoch=96\n",
      "global_step=20832\n",
      "Epoch: 96, Loss: 0.020174015313386917, MSE: 0.0175120710831033, KL: 1.3854003399952288\n",
      "epoch=97\n",
      "global_step=21049\n",
      "Epoch: 97, Loss: 0.016574421897530556, MSE: 0.01754486504429524, KL: 1.4513380382585963\n",
      "epoch=98\n",
      "global_step=21266\n",
      "Epoch: 98, Loss: 0.01666802540421486, MSE: 0.01750035831328789, KL: 1.4822237631733264\n",
      "epoch=99\n",
      "global_step=21483\n",
      "Epoch: 99, Loss: 0.021254763007164, MSE: 0.017651783896251282, KL: 1.286008590883618\n",
      "Saving final model checkpoint...\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = AutoEncoder(emb_channels=8, z_channels=4)\n",
    "# model = ConvVAE()\n",
    "model = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "mse_loss_func = torch.nn.MSELoss()\n",
    "kl_loss_coef = 1e-3\n",
    "\n",
    "model_checkpoint_folder = Path('./checkpoints_kl2')\n",
    "model_checkpoint_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# disc_start = 50001\n",
    "# kl_weight = 1e-6\n",
    "# disc_weight = 0.5\n",
    "# loss_func = LPIPSWithDiscriminator(disc_start, kl_weight=kl_weight, disc_weight=disc_weight)\n",
    "# optimizer2 = Adam(loss_func.discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'{epoch=}')\n",
    "    print(f'{global_step=}')\n",
    "\n",
    "    epoch_mse_loss = 0\n",
    "    epoch_kl_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(e_data_loader_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # batch = batch['pixel_values'].to(device)\n",
    "        batch = batch[0]\n",
    "\n",
    "        img_out, posterior = model(batch)\n",
    "        # img_out = model(batch)\n",
    "        # loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "        # loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "        pred_weights_dict = {}\n",
    "        for out_ in img_out:\n",
    "            for name, param in out_.named_parameters():\n",
    "                if name not in pred_weights_dict:\n",
    "                    pred_weights_dict[name] = []\n",
    "                pred_weights_dict[name].append(param)\n",
    "                \n",
    "\n",
    "        # loss is MSE loss\n",
    "        mse_loss = 0\n",
    "        for key in pred_weights_dict:\n",
    "            mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), batch[key])\n",
    "\n",
    "        # loss = mse_loss_func(batch, img_out) + kl_loss_coef * posterior.kl().mean()\n",
    "        loss = mse_loss + kl_loss_coef * posterior.kl().mean()\n",
    "        # loss = mse_loss \n",
    "\n",
    "        loss.backward()\n",
    "        # if step % 100 == 0:\n",
    "            # print(f'Loss: {loss.item()}, MSE: {mse_loss.item()}, KL: {posterior.kl().mean().item()}')\n",
    "            # print(f'Loss: {loss.item()}, MSE: {mse_loss.item()}')\n",
    "            # print(f'grad norm: {grad_norm(model)}')\n",
    "        epoch_mse_loss += mse_loss.item()\n",
    "        epoch_kl_loss += posterior.kl().mean().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    global_step += step\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}, MSE: {epoch_mse_loss/len(e_data_loader_train)}, KL: {epoch_kl_loss/len(e_data_loader_train)}')\n",
    "print('Saving final model checkpoint...')\n",
    "torch.save(model.state_dict(), os.path.join(str(model_checkpoint_folder), 'autoencoder.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0054, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out, _ = model(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model2 = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out, _ = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cp = f\"./{model_checkpoint_folder}/autoencoder.pt\"\n",
    "model2.load_state_dict(torch.load(model_cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0060, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out,_ = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model2.decoder.load_state_dict(model.decoder.state_dict())\n",
    "model2.encoder.load_state_dict(model.encoder.state_dict())\n",
    "# model2.encoder = model.encoder\n",
    "# model2.decoder = model.decoder\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0066, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c437a21b275005b244436c232defa84105462a85d5e3990796798eae336d0140"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/shashank/research/qd/diffusion_models'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "project_root = os.path.join(str(Path.home()), 'research/qd/diffusion_models')\n",
    "os.chdir(project_root)\n",
    "%pwd # should be PPGA root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam\n",
    "from autoencoders.transformer_autoencoder import AutoEncoder\n",
    "from autoencoders.policy_hyperautoencoder import HyperAutoEncoder\n",
    "from autoencoders.conv_autoencoder import AutoEncoder as ConvVAE\n",
    "from dataset.mnist_fashion_dataset import dataloader\n",
    "from losses.loss_functions import normal_kl\n",
    "from losses.contperceptual import LPIPSWithDiscriminator\n",
    "\n",
    "from dataset.policy_dataset import e_data_loader_train, actor_cfg\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "global_step=0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m# batch = batch['pixel_values'].to(device)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m batch \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m img_out, posterior \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     38\u001b[0m \u001b[39m# img_out = model(batch)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m# loss = loss_func(batch, img_out, posterior, global_step, 0)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# loss += loss_func(batch, img_out, posterior, global_step, 1)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m pred_weights_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/qd/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/autoencoders/policy_hyperautoencoder.py:190\u001b[0m, in \u001b[0;36mHyperAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m z \u001b[39m=\u001b[39m posterior\u001b[39m.\u001b[39msample()\n\u001b[1;32m    189\u001b[0m \u001b[39m# out = self.decode(moment)\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(z)\n\u001b[1;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m out, posterior\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/autoencoders/policy_hyperautoencoder.py:183\u001b[0m, in \u001b[0;36mHyperAutoEncoder.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    181\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_quant_conv(z)\n\u001b[1;32m    182\u001b[0m \u001b[39m# Decode the image of shape `[batch_size, channels, height, width]`\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder([ \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdummy_actor() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(z\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])], z)\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/autoencoders/policy_hyperautoencoder.py:183\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_quant_conv(z)\n\u001b[1;32m    182\u001b[0m \u001b[39m# Decode the image of shape `[batch_size, channels, height, width]`\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder([ \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdummy_actor() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(z\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])], z)\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/autoencoders/policy_hyperautoencoder.py:157\u001b[0m, in \u001b[0;36mHyperAutoEncoder.__init__.<locals>.make_actor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_actor\u001b[39m():\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m Actor(actor_cfg, obs_shape\u001b[39m=\u001b[39;49mactor_cfg\u001b[39m.\u001b[39;49mobs_shape[\u001b[39m0\u001b[39;49m], action_shape\u001b[39m=\u001b[39;49mactor_cfg\u001b[39m.\u001b[39;49maction_shape, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/research/qd/diffusion_models/RL/actor_critic.py:28\u001b[0m, in \u001b[0;36mActor.__init__\u001b[0;34m(self, cfg, obs_shape, action_shape, deterministic)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(cfg)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeterministic \u001b[39m=\u001b[39m deterministic\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_mean \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     24\u001b[0m     layer_init(nn\u001b[39m.\u001b[39mLinear(np\u001b[39m.\u001b[39marray(obs_shape)\u001b[39m.\u001b[39mprod(), \u001b[39m128\u001b[39m)),\n\u001b[1;32m     25\u001b[0m     nn\u001b[39m.\u001b[39mTanh(),\n\u001b[1;32m     26\u001b[0m     layer_init(nn\u001b[39m.\u001b[39mLinear(\u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)),\n\u001b[1;32m     27\u001b[0m     nn\u001b[39m.\u001b[39mTanh(),\n\u001b[0;32m---> 28\u001b[0m     layer_init(nn\u001b[39m.\u001b[39;49mLinear(\u001b[39m128\u001b[39;49m, np\u001b[39m.\u001b[39;49mprod(action_shape)), std\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m),\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeterministic:\n\u001b[1;32m     32\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_logstd \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, np\u001b[39m.\u001b[39mprod(action_shape)))\n",
      "File \u001b[0;32m~/miniconda3/envs/qd/lib/python3.9/site-packages/torch/nn/modules/linear.py:98\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((out_features, in_features), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty(out_features, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = AutoEncoder(emb_channels=8, z_channels=4)\n",
    "# model = ConvVAE()\n",
    "model = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "mse_loss_func = torch.nn.MSELoss()\n",
    "kl_loss_coef = 1e-6\n",
    "\n",
    "model_checkpoint_folder = Path('./checkpoints')\n",
    "model_checkpoint_folder.mkdir(exist_ok=True)\n",
    "\n",
    "disc_start = 50001\n",
    "kl_weight = 1e-6\n",
    "disc_weight = 0.5\n",
    "# loss_func = LPIPSWithDiscriminator(disc_start, kl_weight=kl_weight, disc_weight=disc_weight)\n",
    "# optimizer2 = Adam(loss_func.discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'{epoch=}')\n",
    "    print(f'{global_step=}')\n",
    "\n",
    "    epoch_mse_loss = 0\n",
    "    epoch_kl_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(e_data_loader_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # batch = batch['pixel_values'].to(device)\n",
    "        batch = batch[0]\n",
    "\n",
    "        img_out, posterior = model(batch)\n",
    "        # img_out = model(batch)\n",
    "        # loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "        # loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "        pred_weights_dict = {}\n",
    "        for out_ in img_out:\n",
    "            for name, param in out_.named_parameters():\n",
    "                if name not in pred_weights_dict:\n",
    "                    pred_weights_dict[name] = []\n",
    "                pred_weights_dict[name].append(param)\n",
    "                \n",
    "\n",
    "        # loss is MSE loss\n",
    "        mse_loss = 0\n",
    "        for key in pred_weights_dict:\n",
    "            mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), batch[key])\n",
    "\n",
    "        # loss = mse_loss_func(batch, img_out) + kl_loss_coef * posterior.kl().mean()\n",
    "        loss = mse_loss + kl_loss_coef * posterior.kl().mean()\n",
    "        # loss = mse_loss \n",
    "\n",
    "        loss.backward()\n",
    "        # if step % 100 == 0:\n",
    "            # print(f'Loss: {loss.item()}, MSE: {mse_loss.item()}, KL: {posterior.kl().mean().item()}')\n",
    "            # print(f'Loss: {loss.item()}, MSE: {mse_loss.item()}')\n",
    "            # print(f'grad norm: {grad_norm(model)}')\n",
    "        epoch_mse_loss += mse_loss.item()\n",
    "        epoch_kl_loss += posterior.kl().mean().item()\n",
    "        optimizer.step()\n",
    "\n",
    "    global_step += step\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}, MSE: {epoch_mse_loss/len(e_data_loader_train)}, KL: {epoch_kl_loss/len(e_data_loader_train)}')\n",
    "print('Saving final model checkpoint...')\n",
    "torch.save(model.state_dict(), os.path.join(str(model_checkpoint_folder), 'autoencoder.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0094, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out, _ = model(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model2 = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cp = f\"./{model_checkpoint_folder}/autoencoder.pt\"\n",
    "model2.load_state_dict(torch.load(model_cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0086, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out,_ = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = HyperAutoEncoder(actor_cfg, emb_channels=8, z_channels=4)\n",
    "model2.decoder.load_state_dict(model.decoder.state_dict())\n",
    "model2.encoder.load_state_dict(model.encoder.state_dict())\n",
    "# model2.encoder = model.encoder\n",
    "# model2.decoder = model.decoder\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0066, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x,y = e_data_loader_train.__iter__().__next__()\n",
    "img_out = model2(sampled_x)\n",
    "# loss = loss_func(batch, img_out, posterior, global_step, 0)\n",
    "# loss += loss_func(batch, img_out, posterior, global_step, 1)\n",
    "pred_weights_dict = {}\n",
    "for out_ in img_out:\n",
    "    for name, param in out_.named_parameters():\n",
    "        if name not in pred_weights_dict:\n",
    "            pred_weights_dict[name] = []\n",
    "        pred_weights_dict[name].append(param)\n",
    "        \n",
    "\n",
    "# loss is MSE loss\n",
    "mse_loss = 0\n",
    "for key in pred_weights_dict:\n",
    "    mse_loss += F.mse_loss(torch.stack(pred_weights_dict[key]), sampled_x[key])\n",
    "\n",
    "mse_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c437a21b275005b244436c232defa84105462a85d5e3990796798eae336d0140"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
